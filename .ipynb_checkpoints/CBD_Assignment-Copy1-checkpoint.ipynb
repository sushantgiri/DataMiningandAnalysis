{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2850b1",
   "metadata": {},
   "source": [
    "# CBD Assingment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c56f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dfe916",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"8JRXQtOMNh0P9JEbT0JmkL1PC\"\n",
    "api_key_secret = \"FmeMtZH1fcPc1SVQHyNvhtyKEufP7PqjpBCyWVIliV5RwlExBr\"\n",
    "\n",
    "access_token = \"1541530167922794498-ulpRvGM0tr8dhmoFapLh5d6dLJd1TQ\"\n",
    "access_token_secret = \"tRpCjoFbQKaX4qSb6u0liEKrsW6Hmy1nDMFaPYBlkJuOd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283e66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c0741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f796d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords = ['Altcoin', 'Bitcoin', 'Coindesk', 'Cryptocurrency', 'Gold', 'APPL', 'GOOG', 'YHOO']\n",
    "keywords = ['YHOO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb49a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#YHOO -filter:retweets\n",
      "Created YHOO_2022_07_08 csv file\n"
     ]
    }
   ],
   "source": [
    "date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "for query in keywords:\n",
    "    keywords = keywords\n",
    "    search_query = \"#\"+query+\" \"\"-filter:retweets\"\n",
    "    print(search_query)\n",
    "    \n",
    "    search_tweets = tweepy.Cursor(api.search_tweets,\n",
    "                              q=search_query,\n",
    "                              lang=\"en\").items(1000)\n",
    "    \n",
    "    columns = ['tweet id', 'time of tweet', 'user id', 'text']\n",
    "    data = []\n",
    "    \n",
    "    for tweet in search_tweets:\n",
    "        data.append([tweet.id, tweet.created_at, tweet.user.id, tweet.text])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "           \n",
    "    directory = os.getcwd()\n",
    "        \n",
    "    path = f\"{directory}/{query}/{query}_{date}.csv\"\n",
    "    \n",
    "    df.to_csv(path, index=False)\n",
    "    \n",
    "    print(f\"Created {query}_{date} csv file\")\n",
    "              \n",
    "    #time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b12741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultant CSV after joining all CSV files at a particular location...\n",
      "               tweet id              time of tweet              user id  \\\n",
      "0   1544122064969269250  2022-07-05 00:52:43+00:00  1526095019706302464   \n",
      "1   1544121192046526465  2022-07-05 00:49:15+00:00  1397480691269611521   \n",
      "2   1544120178895720448  2022-07-05 00:45:13+00:00  1397480691269611521   \n",
      "3   1544120160545652736  2022-07-05 00:45:09+00:00  1468280210885951492   \n",
      "4   1544120091310178306  2022-07-05 00:44:52+00:00  1446539506753187841   \n",
      "5   1544119143560912896  2022-07-05 00:41:06+00:00   872855981335601153   \n",
      "6   1544118315483729925  2022-07-05 00:37:49+00:00  1376954378595180550   \n",
      "7   1544117952848338944  2022-07-05 00:36:22+00:00  1448731099996336132   \n",
      "8   1544117882967101440  2022-07-05 00:36:06+00:00  1393116955406655489   \n",
      "9   1544122064969269250  2022-07-05 00:52:43+00:00  1526095019706302464   \n",
      "10  1544121192046526465  2022-07-05 00:49:15+00:00  1397480691269611521   \n",
      "11  1544120178895720448  2022-07-05 00:45:13+00:00  1397480691269611521   \n",
      "12  1544120160545652736  2022-07-05 00:45:09+00:00  1468280210885951492   \n",
      "13  1544120091310178306  2022-07-05 00:44:52+00:00  1446539506753187841   \n",
      "14  1544119143560912896  2022-07-05 00:41:06+00:00   872855981335601153   \n",
      "15  1544118315483729925  2022-07-05 00:37:49+00:00  1376954378595180550   \n",
      "16  1544117952848338944  2022-07-05 00:36:22+00:00  1448731099996336132   \n",
      "17  1544117882967101440  2022-07-05 00:36:06+00:00  1393116955406655489   \n",
      "\n",
      "                                                 text  \n",
      "0   As news of the shooting spread, so did anxiety...  \n",
      "1   [救넖잺游리 NEW LISTING救넖잺]\\n[MILKY/] BEING LISTED IN ...  \n",
      "2   [救넖잺游리 NEW LISTING救넖잺]\\n[MILKY] BEING LISTED IN C...  \n",
      "3   #EverRise $RISE #EverRiseV3 #EverRevoke #DeFi ...  \n",
      "4   Is this a bull trap or we are starting to pump...  \n",
      "5   #Iotex, #Jasmy and #Amp Price Prediction\\n\\n#a...  \n",
      "6   XRP: Analyzing the firmness of this defending ...  \n",
      "7   honestly, it's time to sell your shitcoins and...  \n",
      "8   The #future is near. So we continue building. ...  \n",
      "9   As news of the shooting spread, so did anxiety...  \n",
      "10  [救넖잺游리 NEW LISTING救넖잺]\\n[MILKY/] BEING LISTED IN ...  \n",
      "11  [救넖잺游리 NEW LISTING救넖잺]\\n[MILKY] BEING LISTED IN C...  \n",
      "12  #EverRise $RISE #EverRiseV3 #EverRevoke #DeFi ...  \n",
      "13  Is this a bull trap or we are starting to pump...  \n",
      "14  #Iotex, #Jasmy and #Amp Price Prediction\\n\\n#a...  \n",
      "15  XRP: Analyzing the firmness of this defending ...  \n",
      "16  honestly, it's time to sell your shitcoins and...  \n",
      "17  The #future is near. So we continue building. ...  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for query in keywords:\n",
    "    # setting the path for joining multiple files\n",
    "    files = os.path.join(f\"{directory}/{query}\", \"*.csv\")\n",
    "\n",
    "    # list of merged files returned\n",
    "    files = glob.glob(files)\n",
    "\n",
    "    print(\"Resultant CSV after joining all CSV files at a particular location...\");\n",
    "\n",
    "    # joining files with concat and read_csv\n",
    "    df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57baf718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
